{"cells":[{"cell_type":"markdown","metadata":{"id":"ka9O00omSTel"},"source":["## Load Environment & Detectron 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bu8PGvtFA9LU"},"outputs":[],"source":["!git clone https://github.com/airsplay/py-bottom-up-attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mu0j4EPEBG-9"},"outputs":[],"source":["import os\n","os.chdir(f'./py-bottom-up-attention')\n","\n","!pip install -r requirements.txt\n","!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjRo8WEVN_zt"},"outputs":[],"source":["!python setup.py build develop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_zdiK4-T4zs"},"outputs":[],"source":["import os\n","import io\n","\n","import detectron2\n","\n","# import some common detectron2 utilities\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog\n","\n","# import some common libraries\n","import numpy as np\n","import cv2\n","import torch\n","\n","# Show the image in ipynb\n","from IPython.display import clear_output, Image, display\n","import PIL.Image\n","def showarray(a, fmt='jpeg'):\n","    a = np.uint8(np.clip(a, 0, 255))\n","    f = io.BytesIO()\n","    PIL.Image.fromarray(a).save(f, fmt)\n","    display(Image(data=f.getvalue()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6dt1gFZU8IZ"},"outputs":[],"source":["# Load VG Classes\n","data_path = '/content/py-bottom-up-attention/demo/data/genome/1600-400-20'\n","\n","vg_classes = []\n","with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n","    for object in f.readlines():\n","        vg_classes.append(object.split(',')[0].lower().strip())\n","\n","MetadataCatalog.get(\"vg\").thing_classes = vg_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwP6X7B5U8Ec"},"outputs":[],"source":["cfg = get_cfg()\n","cfg.merge_from_file(\"/content/py-bottom-up-attention/configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml\")\n","cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\n","cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n","# VG Weight\n","cfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe.pkl\"\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","metadata":{"id":"FWxaTov8SfHf"},"source":["## Load Pic"]},{"cell_type":"markdown","metadata":{"id":"hgOiLCK5QEE9"},"source":["### Sample Pic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZ7Q5Ru9U8A5"},"outputs":[],"source":["im = cv2.imread(\"/content/py-bottom-up-attention/demo/data/images/input.jpg\")\n","im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","showarray(im_rgb)"]},{"cell_type":"markdown","metadata":{"id":"Rl2a8ZLRQH2G"},"source":["### Our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EINGkvxZoCk"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKEwTa1uZSsh"},"outputs":[],"source":["im = cv2.imread(\"/content/drive/MyDrive/Capstone/Labeling/dataset/Bar_graph/one_bar_plots/Bar0.png\")\n","im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","showarray(im_rgb)\n"]},{"cell_type":"markdown","metadata":{"id":"8exkpeb_SjFJ"},"source":["## Use detectron 2 to extract features "]},{"cell_type":"markdown","metadata":{"id":"isiCHbAKSxE5"},"source":["### Sample pic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb2XmfCVU77T"},"outputs":[],"source":["NUM_OBJECTS = 36\n","\n","from torch import nn\n","\n","from detectron2.modeling.postprocessing import detector_postprocess\n","from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n","from detectron2.structures.boxes import Boxes\n","from detectron2.structures.instances import Instances\n","\n","def doit(raw_image, raw_boxes):\n","        # Process Boxes\n","    raw_boxes = Boxes(torch.from_numpy(raw_boxes).cuda())\n","    \n","    with torch.no_grad():\n","        raw_height, raw_width = raw_image.shape[:2]\n","        print(\"Original image size: \", (raw_height, raw_width))\n","        \n","        # Preprocessing\n","        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n","        print(\"Transformed image size: \", image.shape[:2])\n","        \n","        # Scale the box\n","        new_height, new_width = image.shape[:2]\n","        scale_x = 1. * new_width / raw_width\n","        scale_y = 1. * new_height / raw_height\n","        #print(scale_x, scale_y)\n","        boxes = raw_boxes.clone()\n","        boxes.scale(scale_x=scale_x, scale_y=scale_y)\n","        \n","        # ----\n","        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n","        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n","        images = predictor.model.preprocess_image(inputs)\n","        \n","        # Run Backbone Res1-Res4\n","        features = predictor.model.backbone(images.tensor)\n","        \n","        # Run RoI head for each proposal (RoI Pooling + Res5)\n","        proposal_boxes = [boxes]\n","        features = [features[f] for f in predictor.model.roi_heads.in_features]\n","        box_features = predictor.model.roi_heads._shared_roi_transform(\n","            features, proposal_boxes\n","        )\n","        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n","        print('Pooled features size:', feature_pooled.shape)\n","        \n","        # Predict classes and boxes for each proposal.\n","        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n","        print(pred_class_logits.shape)\n","        pred_class_prob = nn.functional.softmax(pred_class_logits, -1)\n","        pred_scores, pred_classes = pred_class_prob[..., :-1].max(-1)\n","        \n","        # Detectron2 Formatting (for visualization only)\n","        roi_features = feature_pooled\n","        instances = Instances(\n","            image_size=(raw_height, raw_width),\n","            pred_boxes=raw_boxes,\n","            scores=pred_scores,\n","            pred_classes=pred_classes\n","        )\n","        \n","        return instances, roi_features\n","    \n","given_boxes = np.array(\n","    [[1.7333e+02, 2.1515e+02, 4.8672e+02, 4.7373e+02],\n","        [1.2166e+02, 2.0614e+02, 3.4905e+02, 4.8000e+02],\n","        [5.8896e+02, 0.0000e+00, 6.3909e+02, 3.6998e+02],\n","        [6.0792e+02, 9.0849e+01, 6.3765e+02, 4.2150e+02],\n","        [2.8171e+02, 1.6275e+02, 3.2904e+02, 1.9557e+02],\n","        [1.5337e+02, 9.6636e+01, 3.9307e+02, 4.5865e+02],\n","        [3.9510e+00, 3.0141e-01, 1.7087e+02, 3.7003e+02],\n","        [2.0478e+02, 0.0000e+00, 3.0078e+02, 2.7645e+02],\n","        [3.8164e+02, 3.1898e+02, 6.1028e+02, 4.2289e+02],\n","        [4.2380e+02, 2.7979e+02, 6.3800e+02, 3.9043e+02],\n","        [5.3907e+01, 2.1506e+01, 1.2955e+02, 3.8665e+02],\n","        [2.1639e+02, 3.3180e+02, 4.9085e+02, 4.7821e+02],\n","        [4.5419e+01, 3.1766e+02, 5.8115e+02, 4.7680e+02],\n","        [5.2262e+01, 1.5123e+02, 4.9093e+02, 4.3199e+02],\n","        [3.4266e+02, 4.8674e+01, 6.3398e+02, 3.8901e+02],\n","        [2.4584e+02, 1.8033e+02, 3.4975e+02, 4.0818e+02],\n","        [1.7189e+02, 1.6335e+02, 6.3919e+02, 4.1045e+02],\n","        [1.9629e+01, 0.0000e+00, 5.6497e+02, 1.5761e+02],\n","        [3.9222e+02, 0.0000e+00, 6.3402e+02, 2.7783e+02],\n","        [3.6025e+01, 0.0000e+00, 5.5431e+02, 2.8221e+02],\n","        [1.5994e+02, 1.5115e+00, 3.5376e+02, 3.1772e+02],\n","        [2.9326e+02, 1.4786e+02, 3.2540e+02, 1.8938e+02],\n","        [0.0000e+00, 3.6491e+02, 4.3185e+02, 4.7849e+02],\n","        [1.9907e+01, 4.2409e+02, 4.5854e+02, 4.7957e+02],\n","        [4.9555e+00, 8.1566e+01, 2.3710e+02, 4.5235e+02],\n","        [5.5625e+02, 2.7353e+02, 6.0216e+02, 3.7322e+02],\n","        [9.2086e+01, 2.8328e+02, 3.2548e+02, 4.4708e+02],\n","        [1.7761e+02, 3.6624e+02, 4.5720e+02, 4.6956e+02],\n","        [1.7253e+02, 3.7161e+02, 6.4000e+02, 4.7876e+02],\n","        [2.7954e+02, 2.0651e+02, 3.4036e+02, 3.1626e+02],\n","        [1.9732e+02, 3.8317e+01, 6.4000e+02, 3.2455e+02],\n","        [2.7082e+02, 1.1922e+00, 5.8803e+02, 3.0338e+02],\n","        [6.5850e+00, 1.8703e+02, 3.0191e+02, 4.7954e+02],\n","        [0.0000e+00, 0.0000e+00, 2.2748e+02, 2.3305e+02],\n","        [2.4732e-01, 3.3860e+02, 3.1494e+02, 4.7737e+02],\n","        [2.0554e+02, 1.9619e+00, 6.4000e+02, 2.7667e+02]]\n",")\n","instances, features = doit(im, given_boxes)\n","\n","print(\"Classes\", instances.pred_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wq264HtmU7zN"},"outputs":[],"source":["# Show the boxes, labels, and features\n","pred = instances.to('cpu')\n","v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n","v = v.draw_instance_predictions(pred)\n","showarray(v.get_image()[:, :, ::-1])\n","print('instances:\\n', instances)\n","print()\n","print('boxes:\\n', instances.pred_boxes)\n","print()\n","print('Shape of features:\\n', features.shape)"]},{"cell_type":"markdown","metadata":{"id":"jYA2OT43S1Wl"},"source":["### Our bar chart 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRYqtoCEa4EG"},"outputs":[],"source":["# NUM_OBJECTS = 5\n","\n","from torch import nn\n","\n","from detectron2.modeling.postprocessing import detector_postprocess\n","from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n","from detectron2.structures.boxes import Boxes\n","from detectron2.structures.instances import Instances\n","\n","def doit(raw_image, raw_boxes):\n","        # Process Boxes\n","    raw_boxes = Boxes(torch.from_numpy(raw_boxes).cuda())\n","    \n","    with torch.no_grad():\n","        raw_height, raw_width = raw_image.shape[:2]\n","        print(\"Original image size: \", (raw_height, raw_width))\n","        \n","        # Preprocessing\n","        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n","        print(\"Transformed image size: \", image.shape[:2])\n","        \n","        # Scale the box\n","        new_height, new_width = image.shape[:2]\n","        scale_x = 1. * new_width / raw_width\n","        scale_y = 1. * new_height / raw_height\n","        print(scale_x, scale_y)\n","        boxes = raw_boxes.clone()\n","        boxes.scale(scale_x=scale_x, scale_y=scale_y)\n","        \n","        # ----\n","        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n","        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n","        images = predictor.model.preprocess_image(inputs)\n","        \n","        # Run Backbone Res1-Res4\n","        features = predictor.model.backbone(images.tensor)\n","        \n","        # Run RoI head for each proposal (RoI Pooling + Res5)\n","        proposal_boxes = [boxes]\n","        features = [features[f] for f in predictor.model.roi_heads.in_features]\n","        box_features = predictor.model.roi_heads._shared_roi_transform(\n","            features, proposal_boxes\n","        )\n","        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n","        print('Pooled features size:', feature_pooled.shape)\n","        \n","        # Predict classes and boxes for each proposal.\n","        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n","        print(pred_class_logits.shape)\n","        pred_class_prob = nn.functional.softmax(pred_class_logits, -1)\n","        pred_scores, pred_classes = pred_class_prob[..., :-1].max(-1)\n","        \n","        # Detectron2 Formatting (for visualization only)\n","        roi_features = feature_pooled\n","        instances = Instances(\n","            image_size=(raw_height, raw_width),\n","            pred_boxes=raw_boxes,\n","            scores=pred_scores,\n","            pred_classes=pred_classes\n","        )\n","        \n","        return instances, roi_features\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qaOWJ_eQ11i"},"outputs":[],"source":["given_boxes = np.array(\n","    [[282.575757575758,424.061426956084,503.068036688726,1444.44444444444],\n","    [912.553697898525,342.894619894955,1133.04597701149,1444.44444444444],\n","    [1542.53163822129,296.513618883178,1763.02391733426,1444.44444444444],\n","    [2172.50957854406,203.751506278598,2393.00185765703,1444.44444444444],\n","    [2802.48751886683,134.179894179894,3022.9797979798,1444.44444444444]]\n",")\n","\n","instances, features = doit(im, given_boxes)\n","\n","print(\"Classes\", instances.pred_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhx4RYuLkffg"},"outputs":[],"source":["features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kcf4WzUPif_r"},"outputs":[],"source":["# Show the boxes, labels, and features\n","\n","pred = instances.to('cpu')\n","v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n","v = v.draw_instance_predictions(pred)\n","showarray(v.get_image()[:, :, ::-1])\n","\n","\n","print('instances:\\n', instances)\n","print()\n","print('boxes:\\n', instances.pred_boxes)\n","print()\n","print('Shape of features:\\n', features.shape)"]},{"cell_type":"markdown","metadata":{"id":"lXxoEWPFTBOz"},"source":["## Load bar_box_one_bar coordinates dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Odd71rN7if2p"},"outputs":[],"source":["import json\n","\n","with open('/content/drive/MyDrive/Capstone/Labeling/JSON/dict_bar1_box_all_one_bar_calculated.json') as f:\n","    dict_bar1_box_all_one_bar_calculated = json.load(f) # len = 1592"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G51n5Aw_TabZ"},"outputs":[],"source":["len(dict_bar1_box_all_one_bar_calculated)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1UbR5HBTfj3"},"outputs":[],"source":["dict_bar1_box_all_one_bar_calculated.keys()"]},{"cell_type":"markdown","metadata":{"id":"PKdOygHXTHcN"},"source":["## Construct function to return all one bar features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5n5LySFTTq-P"},"outputs":[],"source":["#test on a different image\n","\n","img_id = 2222\n","chart_file_name = 'Bar'+str(img_id)+'.png'\n","im = cv2.imread(\"/content/drive/MyDrive/Capstone/Labeling/dataset/Bar_graph/one_bar_plots/\"+chart_file_name)\n","given_boxes_test = np.array(list(dict_bar1_box_all_one_bar_calculated[str(img_id)].values()))\n","instances, features = doit(im, given_boxes_test)\n","\n","# Show the boxes, labels, and features\n","pred = instances.to('cpu')\n","v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n","v = v.draw_instance_predictions(pred)\n","showarray(v.get_image()[:, :, ::-1])\n","print('instances:\\n', instances)\n","print()\n","print('boxes:\\n', instances.pred_boxes)\n","print()\n","print('Shape of features:\\n', features.shape)"]},{"cell_type":"markdown","source":["### Same code without prints"],"metadata":{"id":"lqyzDZmwf_-7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTpowvUza0s8"},"outputs":[],"source":["from torch import nn\n","\n","from detectron2.modeling.postprocessing import detector_postprocess\n","from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n","from detectron2.structures.boxes import Boxes\n","from detectron2.structures.instances import Instances\n","\n","def doit(raw_image, raw_boxes):\n","        # Process Boxes\n","    raw_boxes = Boxes(torch.from_numpy(raw_boxes).cuda())\n","    \n","    with torch.no_grad():\n","        raw_height, raw_width = raw_image.shape[:2]\n","        #print(\"Original image size: \", (raw_height, raw_width))\n","        \n","        # Preprocessing\n","        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n","        #print(\"Transformed image size: \", image.shape[:2])\n","        \n","        # Scale the box\n","        new_height, new_width = image.shape[:2]\n","        scale_x = 1. * new_width / raw_width\n","        scale_y = 1. * new_height / raw_height\n","        #print(scale_x, scale_y)\n","        boxes = raw_boxes.clone()\n","        boxes.scale(scale_x=scale_x, scale_y=scale_y)\n","        \n","        # ----\n","        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n","        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n","        images = predictor.model.preprocess_image(inputs)\n","        \n","        # Run Backbone Res1-Res4\n","        features = predictor.model.backbone(images.tensor)\n","        \n","        # Run RoI head for each proposal (RoI Pooling + Res5)\n","        proposal_boxes = [boxes]\n","        features = [features[f] for f in predictor.model.roi_heads.in_features]\n","        box_features = predictor.model.roi_heads._shared_roi_transform(\n","            features, proposal_boxes\n","        )\n","        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n","        #print('Pooled features size:', feature_pooled.shape)\n","        \n","        # Predict classes and boxes for each proposal.\n","        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n","        #print(pred_class_logits.shape)\n","        pred_class_prob = nn.functional.softmax(pred_class_logits, -1)\n","        pred_scores, pred_classes = pred_class_prob[..., :-1].max(-1)\n","        \n","        # Detectron2 Formatting (for visualization only)\n","        roi_features = feature_pooled\n","        instances = Instances(\n","            image_size=(raw_height, raw_width),\n","            pred_boxes=raw_boxes,\n","            scores=pred_scores,\n","            pred_classes=pred_classes\n","        )\n","        \n","        return instances, roi_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X8cdeZHTrIr"},"outputs":[],"source":["one_bar_img_features={}\n","\n","for img_id in dict_bar1_box_all_one_bar_calculated.keys():\n","  chart_file_name = 'Bar'+img_id+'.png'\n","  im = cv2.imread(\"/content/drive/MyDrive/Capstone/Labeling/dataset/Bar_graph/one_bar_plots/\"+chart_file_name)\n","  im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","  \n","  given_boxes = np.array(list(dict_bar1_box_all_one_bar_calculated[img_id].values()))\n","  instances, features = doit(im, given_boxes_test)\n","  one_bar_img_features[img_id] = features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtaL-Tceah8j"},"outputs":[],"source":["len(one_bar_img_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfVmjr50alps"},"outputs":[],"source":["one_bar_img_features['0']"]},{"cell_type":"code","source":[],"metadata":{"id":"cztg8Bp7ikfL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1dCa2wTMlmhQmgWpQoMvSQSyAu3AN6isZ","timestamp":1663585873194}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}