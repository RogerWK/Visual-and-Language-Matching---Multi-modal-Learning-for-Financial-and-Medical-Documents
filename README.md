# Visual-and-Language-Matching---Multi-modal-Learning-for-Financial-and-Medical-Documents


Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of an image usually lacks global semantic concepts as in its corresponding text caption. In this project, it aims to solve the problem of image search and text search from many financial, medical, or even any documents contains images (table or graph) and text (textual description) 

Here is the Google Drive link:
Bar/line chart dataset
https://drive.google.com/drive/folders/1OYa3GdxknDwO6cdzsjUVLUn1HCfr-nLy

Here is the Hugging Face link for LXMERT:
https://huggingface.co/docs/transformers/model_doc/lxmert

Here is the link for image feature extraction:
https://github.com/airsplay/py-bottom-up-attention#feature-extraction
